# বিস্তারিত কোড বিশ্লেষণ এবং লজিক ব্যাখ্যা (Detailed Code Analysis)

এই ডকুমেন্টটিতে `sumo_traffic_rl.py` ফাইলের প্রতিটি লাইনের পেছনের লজিক এবং গণিত সম্পর্কে আরো গভীর আলোচনা করা হলো।

---

### ১. এনভায়রনমেন্ট ম্যানেজমেন্ট (ImprovedSUMOEnv Class)

এটি সিমুলেটর এবং AI এর সংযোগস্থল।

#### **স্টেট রিপ্রেজেন্টেশন (State Representation): `_get_state()`**
স্টেট হলো AI এর 'চোখ'। আমাদের কোডে এটি ২০টি নিউমেরিক্যাল ভ্যালুর একটি অ্যারে:
- **Normalization (স্বাভাবিকীকরণ):** ডেটাগুলোকে নির্দিষ্ট সীমার মধ্যে (যেমন ০ থেকে ১) আনা হয়েছে। যেমন: `queue / 10.0` বা `waiting / 100.0`। এটি নিউরাল নেটওয়ার্ককে দ্রুত শিখতে সাহায্য করে।
- **Lanes:** আমরা প্রধান ৫টি লেনের ডেটা নিচ্ছি। প্রতি লেনের জন্য ৪টি তথ্য: 
    1. ক্যু লেন্থ (গাড়ি দাঁড়িয়ে থাকা)
    2. অপেক্ষার সময়
    3. গাড়ির সংখ্যা
    4. গড় গতিবেগ।

#### **রিওয়ার্ড ক্যালকুলেশন (Reward Logic): `step()`**
রিওয়ার্ড হলো AI এর জন্য 'পুরস্কার' বা 'শাস্তি'। লজিকটি এমনভাবে তৈরি:
- **`old_total_waiting - new_total_waiting`**: যদি ওয়েটিং টাইম কমে, তবে রিওয়ার্ড পজিটিভ হবে।
- **Queue Penalty:** লেনে গাড়ি দাঁড়িয়ে থাকলে বিয়োগাত্মক নম্বর (Negative) দেওয়া হয়, যাতে AI জ্যাম তৈরি করতে ভয় পায়।
- **Bonus:** যদি গড় অপেক্ষার সময় ৫০ সেকেন্ডের নিচে নামাতে পারে, তবে ২০ পয়েন্টের বড় বোনাস দেওয়া হয়।

---

### ২. এজেন্ট আর্কিটেকচার (ImprovedDQNAgent Class)

#### **নিউরাল নেটওয়ার্ক স্ট্রাকচার:**
নেটওয়ার্কটি ৪টি লেয়ার দিয়ে তৈরি: `State (20) → 128 → 128 → 64 → Action (4)`।
- **ReLU Activation:** `_relu = max(0, x)`। এটি নেটওয়ার্ককে জটিল প্যাটার্ন শিখতে সাহায্য করে এবং 'Dead Neuron' সমস্যা কমায়।
- **Target Network Logic:** স্ট্যাবিলিটির জন্য এখানে একটি 'শ্যডো নেটওয়ার্ক' আছে। মেইন নেটওয়ার্ক যখন শিখে, তখন টার্গেট নেটওয়ার্ক স্থির থাকে এবং ১০ ধাপ পরপর নিজেকে আপডেট করে। এটি 'Moving Target' সমস্যা দূর করে।

#### **ব্যকপ্রোপাগেশন (Backpropagation): `_train()`**
এখানেই আসল অংক কাজ করে:
- **Mean Squared Error (MSE) Loss:** AI তার ভুল হিসাব করে এই সূত্রে: `(প্রেডিক্টেড কিউ-ভ্যালু - টার্গেট কিউ-ভ্যালু)²`।
- **Gradient Clipping:** গ্রেডিয়েন্ট যদি খুব বেশি হয়ে যায় (১.০ এর উপরে), তবে আমরা তা কেটে ছোট করে দিই। এতে ট্রেনিংয়ে হঠাৎ কোনো অস্থিরতা (Exploding Gradient) তৈরি হয় না।

---

### ৩. ট্রেনিং মেকানিজম (Training Process)

#### **Epsilon-Decay (শিখার কৌশল)**
- শুরুতে **Epsilon = 1.0 (100%)** থাকে, মানে AI শুধু র‍্যান্ডমলি চেষ্টা করে।
- প্রতিটি এপিসোড শেষে এটি `0.9995` দিয়ে গুণ হয় (কমে যায়)।
- এতে একটা সময় AI র‍্যান্ডম কাজ কমিয়ে তার অর্জিত জ্ঞান ব্যবহার করা শুরু করে। একে বলা হয় **Exploration vs Exploitation Trade-off**।

#### **Experience Replay (অভিজ্ঞতা ঝালাই)**
আমাদের মেমোরি সাইজ ৫০০০। 
- কেন এটা দরকার? কারণ সিগন্যাল পরিবর্তনের ফলাফল সাথে সাথে বোঝা যায় না। মেমোরি থেকে পুরনো ডেটা নিয়ে ট্রেনিং দিলে AI বুঝতে পারে অতীতে সে কি ভুল করেছিল। 
- প্রতি স্টপে আমরা একসাথে ৬৪টি (Batch Size) পুরনো অভিজ্ঞতা নিয়ে ট্রেনিং দেই।

---

### ৪. ফলাফল বিশ্লেষণ (Plotting)
সিমুলেশন শেষে ৪টি গ্রাফ তৈরি হয়:
1. **Total Reward:** এটি সময়ের সাথে বাড়তে থাকা উচিত।
2. **Avg Waiting Time:** এটি সময়ের সাথে কমতে থাকা উচিত (সফলতার প্রধান লক্ষণ)।
3. **Epsilon:** এটি কমতে কমতে নিচে নামবে।
4. **Improvement Bar:** প্রথম ৫ এপিসোড এবং শেষ ৫ এপিসোডের তুলনা দেখায় কত শতাংশ জ্যাম কমেছে।

---
> [!TIP]
> **লজিক টিপস:** আপনি যদি জ্যাম আরো কমাতে চান, তবে `_get_state()`-এ আরো লেনের তথ্য যোগ করতে পারেন অথবা `gamma` (Discount Factor) এর মান বাড়িয়ে ভবিষ্যতের রিওয়ার্ডকে বেশি গুরুত্ব দিতে পারেন।
